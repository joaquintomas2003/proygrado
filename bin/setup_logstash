#!/usr/bin/env bash
set -euo pipefail

LOGSTASH_ETC="${LOGSTASH_ETC:-/etc/logstash}"
ES_HOSTS_JSON="${ES_HOSTS_JSON:-[\"https://localhost:9200\"]}"
ES_USER="${ES_USER:-elastic}"
ES_PASS="${ES_PASS:-diciembre}"
SRC_ES_CA_CERT="${SRC_ES_CA_CERT:-/etc/elasticsearch/certs/http_ca.crt}"
ES_CA_CERT="${ES_CA_CERT:-/etc/logstash/certs/http_ca.crt}"

# ← you asked to call the pipeline "int-metrics"
PIPELINE_ID="${PIPELINE_ID:-int-metrics}"
PORT_BEATS="${PORT_BEATS:-5044}"
HOPS_INDEX_PATTERN="${HOPS_INDEX_PATTERN:-int-metrics-%{+YYYY.MM.dd}}"
EVENT_INDEX_PATTERN="${EVENT_INDEX_PATTERN:-int-event-%{+YYYY.MM.dd}}"
NFP_INDEX_PATTERN="${NFP_INDEX_PATTERN:-nfp-logs-%{+YYYY.MM.dd}}"
PIPELINE_WORKERS="${PIPELINE_WORKERS:-4}"
PQ_BYTES="${PQ_BYTES:-4gb}"

PIPELINES_FILE="$LOGSTASH_ETC/pipelines.yml"
CONF_DIR="$LOGSTASH_ETC/conf.d"
CONF_FILE="$CONF_DIR/30-int-metrics.conf"
LOGSTASH_YML="$LOGSTASH_ETC/logstash.yml"
LS_BIN="/usr/share/logstash/bin/logstash"

ts() { date +%Y%m%d%H%M%S; }

echo "→ Ensuring Logstash config directories exist..."
sudo mkdir -p "$CONF_DIR" /etc/logstash/certs

backup_if_exists () {
  local f="$1"
  if sudo test -f "$f"; then
    local bak="${f}.bak.$(ts)"
    echo "→ Backing up $f to $bak"
    sudo cp -a "$f" "$bak"
  fi
}
backup_if_exists "$PIPELINES_FILE"
backup_if_exists "$CONF_FILE"
backup_if_exists "$LOGSTASH_YML"

# --- Ensure CA cert is present for Logstash ---
if ! sudo test -r "$ES_CA_CERT"; then
  echo "→ Installing CA cert for Logstash..."
  if ! sudo test -r "$SRC_ES_CA_CERT"; then
    echo "✗ Source CA not readable: $SRC_ES_CA_CERT" >&2
    exit 1
  fi
  sudo install -m 0644 "$SRC_ES_CA_CERT" "$ES_CA_CERT"
fi
sudo chmod 0644 "$ES_CA_CERT"
sudo chmod 0755 /etc/logstash /etc/logstash/certs

echo "→ Writing pipelines.yml (overwriting)..."
sudo tee "$PIPELINES_FILE" >/dev/null <<YML
- pipeline.id: ${PIPELINE_ID}
  path.config: "${CONF_DIR}/*.conf"
  pipeline.workers: ${PIPELINE_WORKERS}
  queue.type: persisted
YML

echo "→ Writing ${CONF_FILE} (overwriting)..."
sudo tee "$CONF_FILE" >/dev/null <<CONF
input {
  beats { port => ${PORT_BEATS} }
}

filter {
  if [event][dataset] == "int.metrics" {
    ruby {
      code => '
        latest  = event.get("[int][latest]")  || []
        average = event.get("[int][average]") || []

        # Decode queue_occupancy into objects
        latest.each do |h|
          next unless h.is_a?(Hash) && h["queue_occupancy"]
          val = h["queue_occupancy"].to_i
          h["queue"] = { "id" => (val >> 24) & 0xFF, "occupancy" => val & 0xFFFFFF }
        end

        average.each do |h|
          next unless h.is_a?(Hash) && h["queue_occupancy"]
          val = h["queue_occupancy"].to_i
          h["queue"] = { "id" => (val >> 24) & 0xFF, "occupancy" => val & 0xFFFFFF }
        end

        # --- Flatten INT metric arrays ---
        def pull(arr, key)
          return [] unless arr.is_a?(Array)
          arr.map { |h| h.is_a?(Hash) ? h[key] : nil }.compact
        end

        # Latest → flat arrays
        event.set("latest_node_id",             pull(latest,  "node_id"))
        event.set("latest_hop_latency",         pull(latest,  "hop_latency"))
        event.set("latest_egress_interface_tx", pull(latest,  "egress_interface_tx"))

        # Average → flat arrays
        event.set("average_node_id",             pull(average, "node_id"))
        event.set("average_hop_latency",         pull(average, "hop_latency"))
        event.set("average_egress_interface_tx", pull(average, "egress_interface_tx"))

        # Keep the queues as objects
        event.set("latest_queue_occupancy", latest.map { |h| h["queue"] })
        event.set("average_queue_occupancy", average.map { |h| h["queue"] })

        # Optional: counts for quick QA / visualizations
        event.set("latest_hops_count",  (latest.is_a?(Array)  ? latest.size  : 0))
        event.set("average_hops_count", (average.is_a?(Array) ? average.size : 0))

        # --- Convert flow keys to dotted IP ---
        keys = event.get("[flow][key]")
        if keys && keys.length >= 4
          def u32_to_ip(v)
            v = v.to_i & 0xFFFFFFFF
            "#{(v >> 24) & 0xFF}.#{(v >> 16) & 0xFF}.#{(v >> 8) & 0xFF}.#{v & 0xFF}"
          end
          event.set("[flow][ip_src]", u32_to_ip(keys[0]))
          event.set("[flow][ip_dst]", u32_to_ip(keys[1]))

          src_port = (keys[2] >> 16) & 0xFFFF
          dst_port = keys[2] & 0xFFFF
          event.set("[flow][src_port]", src_port)
          event.set("[flow][dst_port]", dst_port)

          event.set("[flow][proto]", keys[3])
        end

        # --- Compute flow completion time ---
        first_ts = event.get("[flow][first_packet_ts]")
        last_ts  = event.get("[flow][last_update_ts]")

        if first_ts && last_ts
          completion_time = last_ts.to_i - first_ts.to_i
          event.set("[flow][completion_time]", completion_time)
        end

        # --- Process request metadata
        req_metadata = event.get("[int][request_metadata]")
        if req_metadata && req_metadata.is_a?(Integer)
          req_id      = (req_metadata >> 8) & 0xFFFF
          is_response = ((req_metadata >> 7) & 0x1) == 1
          event.set("[int][request_id]", req_id)
          event.set("[int][is_response]", is_response)
        end
      '
    }

    # We intentionally do NOT convert to "integer" to avoid 32-bit overflow.
    # Elasticsearch will store them as numeric (long) from JSON.

    # Drop the bulky originals now that we flattened them:
    mutate {
      remove_field => ["[int][latest]", "[int][average]"]
    }
  }
}

output {
  if [event][dataset] == "int.metrics" {
    elasticsearch {
      hosts                       => ${ES_HOSTS_JSON}
      user                        => "${ES_USER}"
      password                    => "${ES_PASS}"
      ssl_enabled                 => true
      ssl_certificate_authorities => ["${ES_CA_CERT}"]
      index                       => "${HOPS_INDEX_PATTERN}"
    }
  } else if [event][dataset] == "int.event" {
    elasticsearch {
      hosts                       => ${ES_HOSTS_JSON}
      user                        => "${ES_USER}"
      password                    => "${ES_PASS}"
      ssl_enabled                 => true
      ssl_certificate_authorities => ["${ES_CA_CERT}"]
      index                       => "${EVENT_INDEX_PATTERN}"
    }
  } else if [event][dataset] == "nfp.logs" {
    elasticsearch {
      hosts                       => ${ES_HOSTS_JSON}
      user                        => "${ES_USER}"
      password                    => "${ES_PASS}"
      ssl_enabled                 => true
      ssl_certificate_authorities => ["${ES_CA_CERT}"]
      index                       => "${NFP_INDEX_PATTERN}"
    }
  }
}
CONF

echo "→ Enabling persistent queues in logstash.yml (idempotent)..."
if sudo grep -qE '^\s*queue\.type:' "$LOGSTASH_YML"; then
  sudo sed -i 's/^\s*queue\.type:.*/queue.type: persisted/' "$LOGSTASH_YML"
else
  echo "queue.type: persisted" | sudo tee -a "$LOGSTASH_YML" >/dev/null
fi

if sudo grep -qE '^\s*queue\.max_bytes:' "$LOGSTASH_YML"; then
  sudo sed -i "s/^\s*queue\.max_bytes:.*/queue.max_bytes: ${PQ_BYTES}/" "$LOGSTASH_YML"
else
  echo "queue.max_bytes: ${PQ_BYTES}" | sudo tee -a "$LOGSTASH_YML" >/dev/null
fi

echo "→ Validating Logstash configuration..."
sudo -u logstash "$LS_BIN" --path.settings "$LOGSTASH_ETC" --config.test_and_exit

echo "→ Restarting Logstash..."
sudo systemctl restart logstash

echo "→ Tail Logstash logs (Ctrl-C to exit):"
sudo journalctl -u logstash -f
